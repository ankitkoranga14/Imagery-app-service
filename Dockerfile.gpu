# =============================================================================
# GPU Dockerfile for Celery Worker (Optimized)
# =============================================================================
# 
# This Dockerfile builds a GPU-enabled worker for processing:
# - RealESRGAN (4K upscaling)
# - Rembg (background removal)
# - Guardrail ML models (CLIP, YOLO)
#
# Features:
# - Pre-downloaded ML models (no download at runtime)
# - Safetensors format for 4-7x faster weight loading
# - CUDA 12.1 with cuDNN 8
#
# Build: docker build -f Dockerfile.gpu -t imagery-worker-gpu .
# =============================================================================

FROM nvidia/cuda:12.1-cudnn8-runtime-ubuntu22.04 AS base

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    # Model loading optimization
    GUARDRAIL_PARALLEL_LOADING=true \
    GUARDRAIL_BACKGROUND_LOADING=false \
    ML_MODEL_CACHE_DIR=/app/ml_cache

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3-pip \
    build-essential \
    curl \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    && rm -rf /var/lib/apt/lists/* \
    && ln -s /usr/bin/python3.11 /usr/bin/python

WORKDIR /app

# =============================================================================
# Dependencies Stage
# =============================================================================
FROM base AS dependencies

# Copy requirements
COPY requirements.txt requirements-gpu.txt ./

# Install PyTorch with CUDA support first
RUN pip install --no-cache-dir \
    torch==2.1.0+cu121 \
    torchvision==0.16.0+cu121 \
    --extra-index-url https://download.pytorch.org/whl/cu121

# Install other requirements
RUN pip install --no-cache-dir -r requirements.txt

# Install GPU-specific packages
RUN pip install --no-cache-dir -r requirements-gpu.txt || true

# =============================================================================
# Model Download Stage
# =============================================================================
FROM dependencies AS model-downloader

ARG PREDOWNLOAD_MODELS=true

# Copy only what's needed for model download
COPY src/ ./src/
COPY scripts/ ./scripts/

# Create cache directory
RUN mkdir -p /app/ml_cache

# Pre-download and optimize models (STANDARD configuration)
RUN if [ "${PREDOWNLOAD_MODELS}" = "true" ]; then \
        echo "Pre-downloading ML models (STANDARD configuration)..." && \
        python scripts/setup_models.py \
            --cache-dir /app/ml_cache && \
        echo "Models downloaded successfully!" && \
        du -sh /app/ml_cache; \
    else \
        echo "Skipping model pre-download"; \
    fi

# =============================================================================
# Final Stage
# =============================================================================
FROM dependencies AS final

# Copy pre-downloaded models
COPY --from=model-downloader /app/ml_cache /app/ml_cache

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p data/storage

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command for GPU worker
CMD ["celery", "-A", "src.core.celery_app", "worker", "--loglevel=info", "--concurrency=1", "-Q", "gpu_queue"]
