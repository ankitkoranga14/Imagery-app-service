# =============================================================================
# Guardrail Microservice - Docker Compose
# =============================================================================
#
# Lean, production-ready validation service:
# - api: FastAPI server for /validate endpoint
# - worker: GPU-enabled Celery worker for ML inference
# - redis: Celery broker + L0 cache
#
# Models (optimized for validation):
# - Text: all-MiniLM-L6-v2 (sentence-transformers)
# - CLIP: MobileCLIP2-S2 for fast image embeddings (open-clip-torch)
# - YOLO: YOLOv11n for food detection (ultralytics>=8.3.0)
#
# Removed (no longer needed):
# - rembg (background removal)
# - RealESRGAN (upscaling)
# - Nano Banana API integration
#
# Build: docker compose build
# Run:   docker compose up -d
# =============================================================================

services:
  # API Service - FastAPI Application
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PREDOWNLOAD_MODELS: "true"
    container_name: guardrail-api
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - REDIS_URL=redis://redis:6379/1
      - DATABASE_URL=sqlite+aiosqlite:///./data/guardrail.db
      - LOG_LEVEL=INFO
      - LOG_FORMAT_JSON=true
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8000
      # Model loading optimization
      - GUARDRAIL_PARALLEL_LOADING=true
      - GUARDRAIL_BACKGROUND_LOADING=false
      - ML_MODEL_CACHE_DIR=/app/ml_cache
      - CLIP_TEMPERATURE=1.2
    command: gunicorn -k uvicorn.workers.UvicornWorker -w 2 --timeout 300 -b 0.0.0.0:8000 src.main:app
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./food-guard-ui/dist:/app/food-guard-ui/dist
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - guardrail-network

  # Celery Worker - GPU-enabled for ML inference
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
      args:
        PREDOWNLOAD_MODELS: "true"
    container_name: guardrail-worker
    environment:
      - ENVIRONMENT=development
      - REDIS_URL=redis://redis:6379/1
      - DATABASE_URL=sqlite+aiosqlite:///./data/guardrail.db
      - LOG_LEVEL=INFO
      # Model loading optimization
      - GUARDRAIL_PARALLEL_LOADING=true
      - ML_MODEL_CACHE_DIR=/app/ml_cache
      - CLIP_TEMPERATURE=1.2
    command: celery -A src.core.celery_app worker --loglevel=info --concurrency=2 -Q default,validation
    volumes:
      - ./data:/app/data
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - guardrail-network

  # GPU Worker (Optional Profile) - 100% dedicated to YOLOv11 + CLIP
  worker-gpu:
    build:
      context: .
      dockerfile: Dockerfile.worker
      args:
        PREDOWNLOAD_MODELS: "true"
    container_name: guardrail-worker-gpu
    profiles:
      - gpu
    environment:
      - ENVIRONMENT=development
      - REDIS_URL=redis://redis:6379/1
      - DATABASE_URL=sqlite+aiosqlite:///./data/guardrail.db
      - NVIDIA_VISIBLE_DEVICES=all
      # Model loading optimization
      - GUARDRAIL_PARALLEL_LOADING=true
      - ML_MODEL_CACHE_DIR=/app/ml_cache
      - CLIP_TEMPERATURE=1.2
    command: celery -A src.core.celery_app worker --loglevel=info --concurrency=1 -Q gpu_queue,validation
    volumes:
      - ./data:/app/data
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - guardrail-network

  # Redis - Message Broker & L0 Cache
  redis:
    image: redis:7-alpine
    container_name: guardrail-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - guardrail-network

  # Frontend - Validation Dashboard
  frontend:
    build:
      context: ./food-guard-ui
      dockerfile: Dockerfile
    container_name: guardrail-frontend
    ports:
      - "3000:80"
    environment:
      - VITE_API_URL=http://localhost:8000
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - guardrail-network

  # Flower - Celery Monitoring (Optional)
  flower:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: guardrail-flower
    profiles:
      - monitoring
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
    command: celery -A src.core.celery_app flower --port=5555
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - guardrail-network

networks:
  guardrail-network:
    driver: bridge

volumes:
  redis-data:
